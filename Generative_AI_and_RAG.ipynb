{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI and Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "This notebook demonstrates how to implement a RAG pipeline using:\n",
        "\n",
        "- 🗂 A PDF document\n",
        "- 📎 Text chunking\n",
        "- 🔍 Semantic search using embeddings and FAISS\n",
        "- 🧠 A generative model (FLAN-T5)\n",
        "- 📩 Querying for context-aware answers\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XfI2kOgF21Zr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 1: Install Dependencies\n",
        "\n",
        "We install all the libraries needed for PDF loading, chunking, embedding, vector storage, and question-answering."
      ],
      "metadata": {
        "id": "aEFnvD9w3RI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community transformers sentence-transformers faiss-cpu pypdf"
      ],
      "metadata": {
        "id": "NIsjrY75mYwH"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Required Libraries\n",
        "\n",
        "We import LangChain modules for handling PDFs, splitting text, creating embeddings, and storing vectors.\n",
        "We also import Hugging Face `transformers` to load the FLAN-T5 model for text generation.\n"
      ],
      "metadata": {
        "id": "bJfSJ_xj3GTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ],
      "metadata": {
        "id": "ifhY979gx3-Q"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load PDF Document\n",
        "\n",
        "Here we load the `document.pdf` file using `PyPDFLoader` which converts it into a list of documents.\n",
        "Upload your PDF first in the file panel on the left in Colab.\n"
      ],
      "metadata": {
        "id": "4TcZjQIB3i0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"document.pdf\")\n",
        "docs = loader.load()\n",
        "print(f\"Total pages loaded: {len(docs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viW2RYaUx6mg",
        "outputId": "3afcc82a-f844-42db-90f9-2f066844340e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages loaded: 283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Split PDF Content into Chunks\n",
        "\n",
        "We use `RecursiveCharacterTextSplitter` to divide the document into smaller overlapping chunks.\n",
        "\n",
        "- `chunk_size=1000`: each chunk has around 1000 characters.\n",
        "- `chunk_overlap=150`: overlap between chunks for better context continuity.\n",
        "\n",
        "This is necessary because language models have input length limits.\n"
      ],
      "metadata": {
        "id": "_ACQayOz4EAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"Total chunks created: {len(chunks)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ8lB_xWx9FZ",
        "outputId": "464505a1-fd1f-44be-da94-ba049cf96e77"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Generate Embeddings and Create Vector Store\n",
        "\n",
        "We convert each text chunk into a vector using a Sentence-Transformer model: `all-MiniLM-L6-v2`.\n",
        "\n",
        "Then, we store these vectors in a FAISS index — a fast similarity search engine.\n",
        "\n",
        "This allows us to search for semantically similar content later.\n"
      ],
      "metadata": {
        "id": "IXTIU2P04P6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "LkYQm-aKx_bs"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Load FLAN-T5 Large Language Model\n",
        "\n",
        "We use Hugging Face’s `google/flan-t5-large`, a text-to-text model that performs well on reasoning and QA tasks.\n",
        "\n",
        "We wrap it in a `pipeline` for easy querying.\n"
      ],
      "metadata": {
        "id": "D-CMoSWn4bCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "flan_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpw9Nqn8yDyM",
        "outputId": "0792f601-83b6-446f-9ef8-f997a3830ca3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Define a RAG Query Function\n",
        "\n",
        "This function does the following:\n",
        "\n",
        "1. Takes in a user question.\n",
        "2. Uses FAISS retriever to find the most relevant chunks (context).\n",
        "3. Builds a prompt with the context and the question.\n",
        "4. Passes it to FLAN-T5 to generate an answer.\n",
        "\n",
        "This is where Retrieval-Augmented Generation happens!\n"
      ],
      "metadata": {
        "id": "cXEYOjtC4jNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_rag(question):\n",
        "    relevant_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    prompt = f\"Answer the question using only the context:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "    response = flan_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.9,      # Controls creativity\n",
        "        top_k=50,             # Use top 50 most likely tokens\n",
        "        top_p=0.9,            # Use tokens with cumulative probability up to 90%\n",
        "        do_sample=True        # Enables sampling (not greedy)\n",
        "    )\n",
        "\n",
        "    return response[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "TLZctZDeyGwT"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Test the RAG Pipeline\n",
        "\n",
        "Now let's ask a question related to the content of the uploaded document.\n",
        "Try summarizing, extracting facts, or explaining concepts.\n"
      ],
      "metadata": {
        "id": "hFwWGJwc4vNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Summarize the key points of this document in a paragraph of 200 words.\"\n",
        "answer = query_rag(query)\n",
        "print(\" Answer:\\n\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3KcIEYx4rqc",
        "outputId": "a7e6e500-dbd6-4034-cb24-2455ec72e04e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Answer:\n",
            " The Art of Invisibility is a book on becoming invisible when spying and surveillance is now the norm.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Examples"
      ],
      "metadata": {
        "id": "OSZJO8wh_aKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_questions = [\n",
        "    \"What is the main purpose of the book The Art of Invisibility?\",\n",
        "    \"Summarize the key privacy principles discussed in the book.\",\n",
        "    \"Which tools does the book recommend for anonymous browsing?\",\n",
        "    \"How does Kevin Mitnick recommend securing communications and messaging?\",\n",
        "    \"What steps should users take when using public Wi-Fi to stay invisible?\",\n",
        "    \"What advice does the book offer on creating anonymous identities online?\",\n",
        "    \"How are individuals typically tracked online, according to the book?\",\n",
        "    \"What does the book reveal about surveillance by corporations or governments?\",\n",
        "    \"What operating systems or devices are considered most privacy-focused?\",\n",
        "    \"Why is digital privacy important even for people who say they have nothing to hide?\"\n",
        "]\n",
        "\n",
        "for i, question in enumerate(top_questions, start=1):\n",
        "    print(f\"\\n Question {i}: {question}\")\n",
        "    answer = query_rag(question)\n",
        "    print(\" Answer:\\n\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FVGJ9MQ44Eb",
        "outputId": "72d237ff-e804-48a2-c1d5-476c688c97e9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Question 1: What is the main purpose of the book The Art of Invisibility?\n",
            " Answer:\n",
            " to help educate the world's population on how to protect their personal privacy rights from the overstepping of Big Brother and Big Data\n",
            "\n",
            " Question 2: Summarize the key privacy principles discussed in the book.\n",
            " Answer:\n",
            " encrypt and send a secure e-mail protect your data with good password management hide your true IP address from places you visit obscure your computer from being tracked defend your anonymity\n",
            "\n",
            " Question 3: Which tools does the book recommend for anonymous browsing?\n",
            " Answer:\n",
            " Microsoft’s Internet Explorer and Edge\n",
            "\n",
            " Question 4: How does Kevin Mitnick recommend securing communications and messaging?\n",
            " Answer:\n",
            " using PGP or GPG\n",
            "\n",
            " Question 5: What steps should users take when using public Wi-Fi to stay invisible?\n",
            " Answer:\n",
            " iii.\n",
            "\n",
            " Question 6: What advice does the book offer on creating anonymous identities online?\n",
            " Answer:\n",
            " the Tor browser should always be used to create and access all online accounts because it constantly changes your IP address\n",
            "\n",
            " Question 7: How are individuals typically tracked online, according to the book?\n",
            " Answer:\n",
            " a fitness-tracking device such as Fitbit, Jawbone’s UP bracelet, or the Nike+ FuelBand? If not, maybe you wear a smartwatch from Apple, Sony, or Samsung. If you wear one or both of these—a fitness band and/or a smartwatch—you can still be tracked. These devices and their accompanying apps are designed to record your activity, often with GPS information, so whether it is broadcast live or uploaded later, you can that some—if not all—of that data is now online and available to companies that make it their business to collect every bit of personal information off the Internet. The Privacy Rights Clearinghouse lists more than 130 companies that collect personal information (whether or not it’s accurate) about you. And then there’s the data that you don’t volunteer online but that is nonetheless being harvested by corporations and governments— information about whom we e-mail, text, and\n",
            "\n",
            " Question 8: What does the book reveal about surveillance by corporations or governments?\n",
            " Answer:\n",
            " a situation that has become the norm\n",
            "\n",
            " Question 9: What operating systems or devices are considered most privacy-focused?\n",
            " Answer:\n",
            " iOS devices\n",
            "\n",
            " Question 10: Why is digital privacy important even for people who say they have nothing to hide?\n",
            " Answer:\n",
            " (iii)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This assignment provided hands-on experience in building a complete Retrieval-Augmented Generation (RAG) pipeline using LangChain, Hugging Face Transformers, and FAISS. I successfully implemented each step: loading and chunking a PDF document, generating embeddings using Sentence-Transformers, storing them in a FAISS vector store, and using a generative model (FLAN-T5) to provide context-aware answers.\n",
        "\n",
        "### Challenges Faced\n",
        "- **Embedding size limitations**: Handling large document chunks led to memory issues, which were mitigated by tuning `chunk_size` and `chunk_overlap`.\n",
        "- **Model loading latency**: Pre-trained model loading took time, especially with larger models like FLAN-T5.\n",
        "- **Prompt design**: It took a few iterations to engineer prompts that elicited accurate, context-grounded answers.\n",
        "\n",
        "### Remediation Steps\n",
        "- Used `RecursiveCharacterTextSplitter` with optimized parameters to balance chunk size and retrieval accuracy.\n",
        "- Leveraged pipeline caching to reduce model loading delays during repeated queries.\n",
        "- Iteratively refined prompts to improve generation quality and relevance."
      ],
      "metadata": {
        "id": "1SRA-VoxDPZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1Lmdjq_DQhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}